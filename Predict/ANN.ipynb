{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ANN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM8/Np/7TiSMJok2/+daQ+V"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"vYeavbSmNBA_","colab_type":"code","outputId":"87472d9b-0ab2-4cc7-dfc5-71c96e21885d","executionInfo":{"status":"ok","timestamp":1585586618196,"user_tz":-480,"elapsed":27557,"user":{"displayName":"Xinlong Li","photoUrl":"","userId":"00115760002936130732"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"54piIadpNVUg","colab_type":"code","colab":{}},"source":["import os\n","os.chdir(\"/content/gdrive/My Drive/MyAnn\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtJxM9dyNWRj","colab_type":"code","outputId":"2e2d9750-8aeb-4293-d9f0-3bc1519cd88e","executionInfo":{"status":"ok","timestamp":1585586963925,"user_tz":-480,"elapsed":128511,"user":{"displayName":"Xinlong Li","photoUrl":"","userId":"00115760002936130732"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import pandas as pd\n","import numpy\n","import os\n","%tensorflow_version 1.x\n","import tensorflow\n","\n","from sklearn.preprocessing import LabelEncoder\n","from scipy.sparse import csr_matrix, hstack\n","from keras.utils import np_utils\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.wrappers.scikit_learn import KerasClassifier\n","from keras.layers import Dropout\n","from keras.constraints import maxnorm\n","\n","datadir = '/content/gdrive/My Drive/MyAnn/Data'\n","gatrain = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'),index_col='device_id')\n","phone = pd.read_csv(os.path.join(datadir,'phone_brand_device_model.csv'))\n","\n","# Get rid of duplicate device ids in phone\n","phone = phone.drop_duplicates('device_id',keep='first').set_index('device_id')\n","\n","events = pd.read_csv(os.path.join(datadir,'events.csv'),parse_dates=['timestamp'], index_col='event_id')\n","appevents = pd.read_csv(os.path.join(datadir,'app_events.csv'),usecols=['event_id','app_id','is_active'],dtype={'is_active':bool})\n","applabels = pd.read_csv(os.path.join(datadir,'app_labels.csv'))\n","\n","gatrain['trainrow'] = numpy.arange(gatrain.shape[0])\n","\n","# feature engineering\n","\n","# phone brand\n","\n","# encoding\n","brandencoder = LabelEncoder().fit(phone.phone_brand)\n","phone['brand'] = brandencoder.transform(phone['phone_brand'])\n","gatrain['brand'] = phone['brand']\n","\n","# For phone brand data the data array will be all ones, row_ind will be the row number of a device \n","# and col_ind will be the number of brand\n","\n","Xtr_brand = csr_matrix((numpy.ones(gatrain.shape[0]),(gatrain.trainrow, gatrain.brand)))\n","\n","# device model\n","# concatenating phone brand and device model\n","m = phone.phone_brand.str.cat(phone.device_model)\n","\n","# encoding\n","modelencoder = LabelEncoder().fit(m)\n","phone['model'] = modelencoder.transform(m)\n","gatrain['model'] = phone['model']\n","\n","# For device model data the data array will be all ones, row_ind will be the row number of a device \n","# and col_ind will be the number of brand-device model\n","\n","Xtr_model = csr_matrix((numpy.ones(gatrain.shape[0]),(gatrain.trainrow, gatrain.model)))\n","\n","# active apps\n","# merge device_id column from events table to app_events\n","# group the resulting dataframe by device_id and app and aggregate\n","# merge in trainrow and testrow columns to know at which row to put each device in the features matrix\n","\n","# encoding apps\n","appencoder = LabelEncoder().fit(appevents.app_id)\n","appevents['app'] = appencoder.transform(appevents.app_id)\n","napps = len(appencoder.classes_)\n","\n","# merging tables\n","deviceapps = (appevents.merge(events[['device_id']], how='left',left_on='event_id',right_index=True)\n","                       .groupby(['device_id','app'])['app'].agg(['size'])\n","                       .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n","                       .reset_index())\n","\n","# building a feature matrix where the data is all ones, row_ind comes from trainrow or testrow \n","# and col_ind is the label-encoded app_id\n","d = deviceapps.dropna(subset=['trainrow'])\n","Xtr_app = csr_matrix((numpy.ones(d.shape[0]), (d.trainrow, d.app)),shape=(gatrain.shape[0],napps))\n","\n","# app labels\n","# constructed in a way similar to apps features by merging app_labels with the \n","# deviceapps dataframe we created above\n","applabels = applabels.loc[applabels.app_id.isin(appevents.app_id.unique())]\n","\n","# encoding apps in app labels\n","applabels['app'] = appencoder.transform(applabels.app_id)\n","\n","# encoding app labels\n","labelencoder = LabelEncoder().fit(applabels.label_id)\n","applabels['label'] = labelencoder.transform(applabels.label_id)\n","nlabels = len(labelencoder.classes_)\n","\n","# merging\n","devicelabels = (deviceapps[['device_id','app']]\n","                .merge(applabels[['app','label']])\n","                .groupby(['device_id','label'])['app'].agg(['size'])\n","                .merge(gatrain[['trainrow']], how='left', left_index=True, right_index=True)\n","                .reset_index())\n","                \n","# building a feature matrix where the data is all ones, row_ind comes from trainrow or testrow \n","# and col_ind is the encoded label id\n","d = devicelabels.dropna(subset=['trainrow'])\n","Xtr_label = csr_matrix((numpy.ones(d.shape[0]), (d.trainrow, d.label)),shape=(gatrain.shape[0],nlabels))\n","\n","# concatenate all features\n","Xtrain = hstack((Xtr_brand, Xtr_model, Xtr_app, Xtr_label), format='csr')\n","\n","# deleting objects that are no longer needed\n","del phone\n","del appevents\n","del applabels\n","del Xtr_brand\n","del Xtr_model\n","del Xtr_app\n","del Xtr_label\n","\n","MIN_VAL_ALLOWED = 15\n","\n","nsp = numpy.squeeze(numpy.asarray(Xtrain.sum(axis=0) >= MIN_VAL_ALLOWED))\n","Xtrain = Xtrain[:,nsp].toarray()\n","\n","# encoding demographic group\n","targetencoder = LabelEncoder().fit(gatrain.group)\n","encoded_y = targetencoder.transform(gatrain.group)\n","dummy_y = np_utils.to_categorical(encoded_y)\n","nclasses = len(targetencoder.classes_)\n","\n","# model\n","model = Sequential()\n","model.add(Dense(Xtrain.shape[1], input_dim=Xtrain.shape[1], init = 'glorot_normal', activation='softsign', W_constraint=maxnorm(2)))\n","model.add(Dropout(0.6))\n","model.add(Dense(int(Xtrain.shape[1]*0.6), init = 'glorot_normal', activation='softsign', W_constraint=maxnorm(2)))\n","model.add(Dropout(0.6))\n","model.add(Dense(nclasses, init = 'glorot_normal', activation='softmax'))\n","\n","# Compile model\n","model.compile(loss='categorical_crossentropy', optimizer='Adadelta', metrics=['accuracy'])\n","\n","model.fit(Xtrain, dummy_y, nb_epoch=20, batch_size=1000)\n","\n","# evaluate the model\n","scores = model.evaluate(Xtrain, dummy_y)\n","print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n","\n","predictions = model.predict(Xtrain)\n","\n","pred = pd.DataFrame(data = predictions,\n","                    columns = list(targetencoder.inverse_transform(list(set(encoded_y)))))\n","\n","# creating a column to indicate which is our predicted group\n","pred['prediction'] = pred.idxmax(axis=1)\n","\n","# writing to csv\n","pred.to_csv('output_ann.csv',index=True)\n","\n","# reading back the ann and dropping the unnecessary columns\n","predictions = pd.read_csv('output_ann.csv')\n","del predictions['Unnamed: 0']\n","\n","# merging with the test data to see if we've made the right predictions\n","gatrain['device id'] = gatrain.index\n","gatrain = gatrain.set_index('trainrow')\n","eva = gatrain.merge(predictions, left_index=True, right_index=True)\n","eva = eva.set_index('device id')\n","eva = eva[['gender','group','prediction']]\n","eva['predicted gender'] = eva['prediction'].str[0]\n","\n","# comparing actual and predicted gender and group\n","eva['gender result'] = (eva['gender'] == eva['predicted gender']).astype(int)\n","eva['group result'] = (eva['group'] == eva['prediction']).astype(int)\n","\n","# dropping columns that are not needed\n","eva = eva[['prediction','predicted gender','gender result','group result']]\n","\n","# changing column names\n","eva.columns = ['age','gender','gender result','age result']\n","\n","# merging with events table to get latitude and longitude values\n","events = events.set_index('device_id')\n","# first delete all entries in events where lat long is zero or long<70\n","events = events[(events.latitude != 0) & (events.longitude >70)]\n","\n","# then merge and drop duplicate entries\n","final = eva.merge(events, left_index=True, right_index=True, how='inner')\n","final['device id'] = final.index\n","final = final.drop_duplicates('device id',keep='first')\n","\n","# dropping unnecessary columns\n","final = final[['device id','gender','age','latitude','longitude']]\n","\n","# writing result\n","final.to_csv('/content/gdrive/My Drive/MyAnn/Visualization/datafiles/ANN.csv', index=False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n","  mask |= (ar1 == a)\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n","WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:128: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(2658, input_dim=2658, activation=\"softsign\", kernel_initializer=\"glorot_normal\", kernel_constraint=<keras.con...)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:130: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1594, activation=\"softsign\", kernel_initializer=\"glorot_normal\", kernel_constraint=<keras.con...)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:132: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(12, activation=\"softmax\", kernel_initializer=\"glorot_normal\")`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:137: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Epoch 1/20\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","74645/74645 [==============================] - 12s 166us/step - loss: 2.4048 - acc: 0.1438\n","Epoch 2/20\n","74645/74645 [==============================] - 5s 61us/step - loss: 2.3696 - acc: 0.1598\n","Epoch 3/20\n","74645/74645 [==============================] - 5s 61us/step - loss: 2.3573 - acc: 0.1638\n","Epoch 4/20\n","74645/74645 [==============================] - 5s 61us/step - loss: 2.3476 - acc: 0.1707\n","Epoch 5/20\n","74645/74645 [==============================] - 5s 61us/step - loss: 2.3428 - acc: 0.1701\n","Epoch 6/20\n","74645/74645 [==============================] - 5s 61us/step - loss: 2.3351 - acc: 0.1756\n","Epoch 7/20\n","74645/74645 [==============================] - 4s 60us/step - loss: 2.3313 - acc: 0.1761\n","Epoch 8/20\n","74645/74645 [==============================] - 5s 60us/step - loss: 2.3279 - acc: 0.1784\n","Epoch 9/20\n","74645/74645 [==============================] - 4s 60us/step - loss: 2.3236 - acc: 0.1797\n","Epoch 10/20\n","74645/74645 [==============================] - 4s 60us/step - loss: 2.3188 - acc: 0.1813\n","Epoch 11/20\n","74645/74645 [==============================] - 4s 60us/step - loss: 2.3161 - acc: 0.1842\n","Epoch 12/20\n","74645/74645 [==============================] - 4s 60us/step - loss: 2.3125 - acc: 0.1852\n","Epoch 13/20\n","74645/74645 [==============================] - 5s 61us/step - loss: 2.3089 - acc: 0.1874\n","Epoch 14/20\n","74645/74645 [==============================] - 4s 60us/step - loss: 2.3043 - acc: 0.1863\n","Epoch 15/20\n","74645/74645 [==============================] - 5s 60us/step - loss: 2.3024 - acc: 0.1881\n","Epoch 16/20\n","74645/74645 [==============================] - 4s 60us/step - loss: 2.2988 - acc: 0.1902\n","Epoch 17/20\n","74645/74645 [==============================] - 4s 60us/step - loss: 2.2979 - acc: 0.1903\n","Epoch 18/20\n","74645/74645 [==============================] - 4s 60us/step - loss: 2.2934 - acc: 0.1938\n","Epoch 19/20\n","74645/74645 [==============================] - 4s 60us/step - loss: 2.2927 - acc: 0.1927\n","Epoch 20/20\n","74645/74645 [==============================] - 4s 60us/step - loss: 2.2906 - acc: 0.1913\n","74645/74645 [==============================] - 7s 89us/step\n","acc: 20.27%\n"],"name":"stdout"}]}]}